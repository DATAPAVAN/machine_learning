{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Introduction:**\n",
        "\n",
        "**what is supervised learning?**\n",
        "\n",
        "*  **Supervised learning** is a machine learning technique in which an algorithm learns to map input data to known output data by training on a labeled dataset. The goal of supervised learning is to create a model that can predict outputs for new, unseen inputs with a high degree of accuracy.\n",
        "\n",
        "*  Supervised learning involves two main phases: training and testing. In the training phase, the algorithm is given a dataset consisting of input/output pairs and learns to generalize patterns and relationships between them. This is typically done by minimizing a loss function that measures the error between the predicted output and the actual output. The algorithm adjusts its internal parameters (also known as weights) during training to minimize the loss.\n",
        "\n",
        "*  Once the model is trained, it is evaluated on a separate testing dataset to measure its performance on new, unseen inputs. The accuracy of the model is typically measured using performance metrics such as accuracy, precision, recall, F1 score, etc.\n",
        "\n",
        "*  Supervised learning algorithms can be further classified into two categories: \n",
        "\n",
        " regression and classification. \n",
        "   *   Regression algorithms are used to predict continuous output values, such as predicting the price of a house based on its features.\n",
        "   *   Classification algorithms are used to predict discrete output values, such as classifying an email as spam or not spam.\n"
      ],
      "metadata": {
        "id": "5q0fXr8aNh1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Algorithms:**\n",
        "\n",
        "\n",
        "**what are algorithms used in supervised learning?**\n",
        "\n",
        "There are several algorithms used in supervised learning for **regression**:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Linear Regression:** \n",
        "Linear regression is a simple algorithm that finds the best linear fit to the data.\n",
        "\n",
        "**Polynomial Regression:**\n",
        " Polynomial regression is an extension of linear regression that models the relationship between the independent variable and the dependent variable as an nth degree polynomial.\n",
        "\n",
        "**Ridge Regression:**\n",
        " Ridge regression is a regularization technique used to prevent overfitting by adding a penalty term to the loss function.\n",
        "\n",
        "**Lasso Regression:**\n",
        " Lasso regression is another regularization technique that uses an L1 penalty instead of an L2 penalty to encourage sparsity.\n",
        "\n",
        "**Elastic Net Regression:** \n",
        "Elastic net regression is a combination of ridge and lasso regression that uses both L1 and L2 penalties.\n",
        "\n",
        "**Support Vector Regression:**\n",
        " Support vector regression is a regression technique that uses support vector machines to find the best fit to the data.\n",
        "\n",
        "**Decision Tree Regression:**\n",
        " Decision tree regression is a non-parametric algorithm that splits the data based on the values of the independent variables to predict the dependent variable.\n",
        "\n",
        "**Random Forest Regression:**\n",
        " Random forest regression is an ensemble learning algorithm that combines multiple decision trees to improve the prediction accuracy.\n",
        "\n",
        "Here are the common algorithms used in supervised learning for **classification:**\n",
        "\n",
        "**Logistic Regression:** \n",
        "Logistic regression is a statistical method used to analyze a data set in which there are one or more independent variables that determine an outcome. It is commonly used to model binary outcomes (i.e., outcomes with two possible values).\n",
        "\n",
        "**Decision Trees:** Decision trees are a simple but powerful predictive modeling tool that can be used to solve both classification and regression problems. They work by splitting the data into smaller subsets based on the values of the input features, until a leaf node is reached that contains the predicted outcome.\n",
        "\n",
        "**Random Forest:** A random forest is an ensemble of decision trees, where each tree is trained on a randomly selected subset of the training data. This helps to reduce overfitting and improve the accuracy of the model.\n",
        "\n",
        "**Support Vector Machines (SVM):** SVM is a powerful algorithm used for both classification and regression problems. SVM finds a hyperplane (a line or a plane) that separates the classes in the best way possible.\n",
        "\n",
        "**Naive Bayes:** Naive Bayes is a simple yet effective algorithm that is commonly used in text classification and spam filtering. It works on the principle of Bayes' theorem and assumes that the presence of a particular feature in a class is independent of the presence of other features.\n",
        "\n",
        "**K-Nearest Neighbors (KNN):** KNN is a non-parametric algorithm that is used for both classification and regression. It works by finding the K nearest data points in the training set and using their labels to predict the label of the new data point.\n",
        "\n",
        "**Neural Networks:** Neural networks are a powerful class of algorithms inspired by the structure of the human brain. They consist of multiple layers of interconnected nodes (neurons) that learn to extract features from the input data and make predictions.\n"
      ],
      "metadata": {
        "id": "BYsMPUq4UeAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**steps involved in supervised learning:**\n",
        "\n",
        "The process for a problem in supervised learning typically involves the following steps:\n",
        "\n",
        "**Data collection:** Collecting a dataset that is relevant and sufficient to solve the problem at hand. This dataset should include the input features (independent variables) and the corresponding output (dependent variable) that we are trying to predict.\n",
        "\n",
        "**Data preprocessing:** This involves cleaning the data, dealing with missing values, handling outliers, and scaling or normalizing the data. This step is important as the quality of the data affects the performance of the model.\n",
        "\n",
        "**Data splitting:** Splitting the dataset into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate the performance of the model.\n",
        "\n",
        "**Feature engineering:** Selecting and transforming the relevant input features to enhance the performance of the model. This step involves techniques like feature selection, feature extraction, and feature scaling.\n",
        "\n",
        "**Model selection:** Choosing an appropriate model that can best solve the problem at hand. This involves selecting the algorithm and the hyperparameters that will be used to train the model.\n",
        "\n",
        "**Model training:** Training the model on the training set using the selected algorithm and hyperparameters.\n",
        "\n",
        "**Model evaluation:** Evaluating the performance of the trained model on the testing set using appropriate performance metrics.\n",
        "\n",
        "**Model tuning:** Fine-tuning the hyperparameters of the model to achieve better performance. This step involves techniques like grid search, random search, and Bayesian optimization.\n",
        "\n",
        "**Model deployment:** Once the model has been trained and tested, it can be deployed in production to make predictions on new data."
      ],
      "metadata": {
        "id": "L9-9QZAIW3jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Regression:**\n",
        "\n",
        "Regression is a type of supervised learning in machine learning, where the goal is to predict a continuous numeric output variable based on one or more input variables. In regression, the aim is to learn a relationship between the input variables and output variable from the given training data and then use this learned relationship to predict the output values for new input data.\n",
        "\n",
        "*  The main objective of regression is to minimize the difference between the predicted values and the actual values of the output variable. Regression models can be simple or complex, depending on the number of input variables and the type of relationship between the input and output variables.\n",
        "\n",
        "*  Linear regression is the simplest form of regression, where the relationship between the input and output variables is linear. There are also other types of regression models, such as polynomial regression, ridge regression, and lasso regression, which can capture more complex relationships between the input and output variables.\n",
        "\n",
        "*  Regression models are commonly used in many applications, including finance, economics, healthcare, and engineering, to predict future outcomes based on past data. Performance metrics, such as mean squared error (MSE) and root mean squared error (RMSE), are used to evaluate the accuracy of regression models.\n",
        "\n",
        "##**Algorithms used in Regression:**\n",
        "\n",
        "Some of the algorithms used in regression in supervised learning are:\n",
        "\n",
        "*  Linear Regression\n",
        "*  Polynomial Regression\n",
        "*  Ridge Regression\n",
        "*  Lasso Regression\n",
        "*  Elastic Net Regression\n",
        "*  Support Vector Regression (SVR)\n",
        "*  Decision Tree Regressor\n",
        "*  Random Forest Regressor\n",
        "*  Gradient Boosting Regressor\n",
        "*  AdaBoost Regressor\n",
        "*  XGBoost Regressor\n",
        "*  LightGBM Regressor\n",
        "*  CatBoost Regressor\n",
        "\n",
        "###**Performance Metrics Used For Regression:**\n",
        "\n",
        "There are several performance metrics that can be used to evaluate the performance of a linear regression model in supervised learning:\n",
        "\n",
        "**Mean Squared Error (MSE):**\n",
        "\n",
        " MSE measures the average squared difference between the predicted and actual values. It is calculated by taking the average of the squared differences between the predicted and actual values. A lower MSE value indicates better performance of the model.\n",
        "\n",
        "**Root Mean Squared Error (RMSE):**\n",
        "\n",
        " RMSE is the square root of the MSE and provides an interpretable measure of the average error. A lower RMSE value indicates better performance of the model.\n",
        "\n",
        "**Mean Absolute Error (MAE):**\n",
        "\n",
        " MAE measures the average absolute difference between the predicted and actual values. It is calculated by taking the average of the absolute differences between the predicted and actual values. A lower MAE value indicates better performance of the model.\n",
        "\n",
        "**R-squared (R²):**\n",
        "\n",
        " R-squared measures the proportion of variance in the dependent variable that is explained by the independent variables in the model. It is a value between 0 and 1, where 1 indicates a perfect fit and 0 indicates no fit. A higher R-squared value indicates better performance of the model.\n",
        "\n",
        "**Adjusted R-squared:** \n",
        "\n",
        "Adjusted R-squared is similar to R-squared, but it takes into account the number of independent variables in the model. It penalizes the addition of independent variables that do not significantly contribute to the model. A higher adjusted R-squared value indicates better performance of the model.\n",
        "\n",
        "**Mean Squared Percentage Error (MSPE):**\n",
        "\n",
        " MSPE measures the average percentage difference between the predicted and actual values. It is calculated by taking the average of the squared percentage differences between the predicted and actual values. A lower MSPE value indicates better performance of the model.\n",
        "\n",
        "**Mean Absolute Percentage Error (MAPE):**\n",
        "\n",
        " MAPE measures the average absolute percentage difference between the predicted and actual values. It is calculated by taking the average of the absolute percentage differences between the predicted and actual values. A lower MAPE value indicates better performance of the model.\n",
        "\n",
        "\n",
        "\n",
        "##**Questions regarding performance metrics:**\n",
        "\n",
        "**why we have rmse when mse exists?**\n",
        "\n",
        "RMSE (Root Mean Squared Error) and MSE (Mean Squared Error) are both performance metrics used for evaluating regression models. RMSE is just the square root of MSE, so they are closely related.\n",
        "\n",
        "The reason why we might use RMSE instead of MSE is that RMSE has the same unit of measurement as the target variable, whereas MSE has squared units. This can make it easier to interpret the error metric in the context of the problem.\n",
        "\n",
        "For example, suppose we are predicting the price of a house in dollars. If we use MSE as our error metric, the units of the error will be dollars squared. This can be difficult to interpret and communicate to stakeholders. On the other hand, if we use RMSE, the units of the error will be dollars, which is more intuitive and easier to communicate.\n",
        "\n",
        "**why we use mae when rmse and mse exists?**\n",
        "\n",
        "MAE (Mean Absolute Error) is used as a performance metric in machine learning because it has certain advantages over RMSE (Root Mean Squared Error) and MSE (Mean Squared Error) in certain situations.\n",
        "\n",
        "One advantage of MAE is that it is more interpretable than RMSE and MSE, as it represents the average absolute difference between the predicted and actual values. This means that the MAE value can be easily understood in the context of the problem, whereas RMSE and MSE values are not as intuitive to interpret.\n",
        "\n",
        "Another advantage of MAE is that it is less sensitive to outliers than RMSE and MSE. This is because MAE only considers the absolute differences between the predicted and actual values, whereas RMSE and MSE also consider the squared differences, which can be greatly influenced by outliers. Therefore, if the dataset contains outliers, using MAE as a performance metric may be more appropriate.\n",
        "\n",
        "**why we need r squared when we have other metrics like mse explain clearly?**\n",
        "\n",
        "While metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) measure the performance of a regression model by looking at the difference between the predicted values and the actual values, they do not provide an indication of how well the model fits the data relative to a simple average model.\n",
        "\n",
        "R-squared (R²) is a metric that provides a measure of how well the regression model fits the data by comparing the residual variance of the model to the residual variance of a simple average model. It is a statistical measure that represents the proportion of the variance in the dependent variable that can be explained by the independent variables in the model.\n",
        "\n",
        "In other words, R-squared tells us how much of the variation in the response variable is explained by the variation in the predictor variables. It ranges from 0 to 1, with higher values indicating a better fit of the model to the data.\n",
        "\n",
        "Therefore, R-squared is a useful metric to assess the goodness-of-fit of a regression model and provides additional information about the model's performance beyond what is provided by metrics like MSE, RMSE, and MAE.\n",
        "\n"
      ],
      "metadata": {
        "id": "hPLNrGGsYFQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Linear Regression:**\n",
        "\n",
        "Understanding Linear Regression\n",
        "In the most simple words, Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
        "\n",
        "**Linear Regression is of two types:** \n",
        "\n",
        "Simple and Multiple. Simple Linear Regression is where only one independent variable is present and the model has to find the linear relationship of it with the dependent variable\n",
        "\n",
        "Whereas, In Multiple Linear Regression there are more than one independent variables for the model to find the relationship.\n",
        "\n",
        "Equation of Simple Linear Regression, where bo is the intercept, b1 is coefficient or slope, x is the independent variable and y is the dependent variable.\n",
        "\n",
        "\n",
        "Equation of Multiple Linear Regression, where bo is the intercept, b1,b2,b3,b4…,bn are coefficients or slopes of the independent variables x1,x2,x3,x4…,xn and y is the dependent variable.\n",
        "\n",
        "A Linear Regression model’s main aim is to find the best fit linear line and the optimal values of intercept and coefficients such that the error is minimized.\n",
        "Error is the difference between the actual value and Predicted value and the goal is to reduce this difference.\n",
        "\n",
        "**Let’s understand this with the help of a diagram:**\n",
        "\n",
        "The blue line is the best fit line predicted by the model i.e the predicted values lie on the blue line.\n",
        "The vertical distance between the data point and the regression line is known as error or residual. Each data point has one residual and the sum of all the differences is known as the Sum of Residuals/Errors. \n",
        "\n",
        "**Mathematical Approach:**\n",
        "\n",
        "Residual/Error = Actual values – Predicted Values\n",
        "\n",
        "Sum of Residuals/Errors = Sum(Actual- Predicted Values)\n",
        "\n",
        "Square of Sum of Residuals/Errors = (Sum(Actual- Predicted Values))2\n",
        "\n",
        "\n",
        "\n",
        "**Assumptions of Linear Regression\n",
        "The basic assumptions of Linear Regression are as follows:**\n",
        "\n",
        "1. Linearity: It states that the dependent variable Y should be linearly related to independent variables. This assumption can be checked by plotting a scatter plot between both variables.\n",
        "\n",
        " \n",
        "\n",
        "2. Normality: The X and Y variables should be normally distributed. Histograms, KDE plots, Q-Q plots can be used to check the Normality assumption. \n",
        "\n",
        "Please refer to my attached blog for a detailed explanation on checking the normality and transforming the variables violating the assumption.\n",
        "\n",
        "Assumptions of Linear Regression 2\n",
        "Source: https://heljves.com/gallery/vol_1_issue_1_2019_8.pdf\n",
        "\n",
        "3. Homoscedasticity: The variance of the error terms should be constant i.e the spread of residuals should be constant for all values of X. This assumption can be checked by plotting a residual plot. If the assumption is violated then the points will form a funnel shape otherwise they will be constant.\n",
        "\n",
        "\n",
        "\n",
        "4. Independence/No Multicollinearity: The variables should be independent of each other i.e no correlation should be there between the independent variables. To check the assumption, we can use a correlation matrix or VIF score. If the VIF score is greater than 5 then the variables are highly correlated.\n",
        "\n",
        "\n",
        "5. The error terms should be normally distributed. Q-Q plots and Histograms can be used to check the distribution of error terms.\n",
        "\n",
        "\n",
        "\n",
        "6. No Autocorrelation: The error terms should be independent of each other. Autocorrelation can be tested using the Durbin Watson test. The null hypothesis assumes that there is no autocorrelation. The value of the test lies between 0 to 4. If the value of the test is 2 then there is no autocorrelation.\n",
        "\n",
        "\n",
        "\n",
        "**How to deal with the Violation of any of the Assumption:**\n",
        "\n",
        "\n",
        "The Violation of the assumptions leads to a decrease in the accuracy of the model therefore the predictions are not accurate and error is also high.\n",
        "\n",
        "For example, if the Independence assumption is violated then the relationship between the independent and dependent variable can not be determined precisely.\n",
        "\n",
        "There are various methods are techniques available to deal with the violation of the assumptions. Let’s discuss some of them below.\n",
        "\n",
        "**Violation of Normality assumption of variables or error terms:**\n",
        "\n",
        "To treat this problem, we can transform the variables to the normal distribution using various transformation functions such as log transformation, Reciprocal, or Box-Cox Transformation.\n",
        "All the functions are discussed in this article of mine: How to transform into Normal Distribution\n",
        "\n",
        "**Violation of MultiCollineraity Assumption \n",
        "It can be dealt with by:**\n",
        "\n",
        "Doing nothing (if there is no major difference in the accuracy)\n",
        "\n",
        "Removing some of the highly correlated independent variables.\n",
        "\n",
        "Deriving a new feature by linearly combining the independent variables, such as adding them together or performing some mathematical operation.\n",
        "\n",
        "Performing an analysis designed for highly correlated variables, such as principal components analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RBPnluisYP1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Polynomial Regression:**\n",
        "\n",
        "\n",
        "**what is polynomial regression?**\n",
        "\n",
        "In polynomial regression, the relationship between the independent variable x and the dependent variable y is described as an nth degree polynomial in x. Polynomial regression, abbreviated E(y |x), describes the fitting of a nonlinear relationship between the value of x and the conditional mean of y. It usually corresponded to the least-squares method. According to the Gauss Markov Theorem, the least square approach minimizes the variance of the coefficients. This is a type of Linear Regression in which the dependent and independent variables have a curvilinear relationship and the polynomial equation is fitted to the data; we’ll go over that in more detail later in the article. Machine learning is also referred to as a subset of Multiple Linear Regression. Because we convert the Multiple Linear Regression equation into a Polynomial Regression equation by including more polynomial elements.\n",
        "\n",
        "**Types of Polynomial Regression:**\n",
        "\n",
        "A quadratic equation is a general term for a second-degree polynomial equation. This degree, on the other hand, can go up to nth values. Polynomial regression can so be categorized as follows:\n",
        "\n",
        "1. Linear – if degree as 1\n",
        "\n",
        "2. Quadratic – if degree as 2\n",
        "\n",
        "3. Cubic – if degree as 3 and goes on, on the basis of degree.\n",
        "\n",
        " \n",
        "**Assumption of Polynomial Regression:**\n",
        "\n",
        "*  We cannot process all of the datasets and use polynomial regression machine learning to make a better judgment. We can still do it, but there should be specific constraints for the dataset in order to get the best polynomial regression results.\n",
        "*  A dependent variable’s behaviour can be described by a linear, or curved, an additive link between the dependent variable and a set of k independent factors.\n",
        "The independent variables have no relationship with one another.\n",
        "We’re utilizing datasets with independent errors that are normally distributed with a mean of zero and a constant variance.\n",
        "\n",
        "**Simple math to understand Polynomial Regression:**\n",
        "\n",
        "Here we are dealing with mathematics, rather than going deep, just understand the basic structure, we all know the equation of a linear equation will be a straight line, from that if we have many features then we opt for multiple regression just increasing features part alone, then how about polynomial, it’s not about increasing but changing the structure to a quadratic equation, you can visually understand from the diagram,\n",
        "\n",
        "**Maths behind Polynomial Regression:**\n",
        "\n",
        "Linear Regression Vs Polynomial Regression:\n",
        "\n",
        "*  Rather than focusing on the distinctions between linear and polynomial regression, we may comprehend the importance of polynomial regression by starting with linear regression. We build our model and realize that it performs abysmally. We examine the difference between the actual value and the best fit line we predicted, and it appears that the true value has a curve on the graph, but our line is nowhere near cutting the mean of the points. This is where polynomial regression comes into play; it predicts the best-fit line that matches the pattern of the data (curve).\n",
        "\n",
        "*  One important distinction between Linear and Polynomial Regression is that Polynomial Regression does not require a linear relationship between the independent and dependent variables in the data set. When the Linear Regression Model fails to capture the points in the data and the Linear Regression fails to adequately represent the optimum conclusion, Polynomial Regression is used.\n",
        "Before delving into the topic, let us first understand why we prefer Polynomial Regression over Linear Regression in some situations, say the non-linear condition of the dataset, by programming and visualization.\n",
        "Python Code:\n",
        "\n",
        "\n",
        "And now we do regression analysis, in particular, Linear Regression, and see how well our random data gets analyzed perfectly.\n",
        "\n",
        "```\n",
        "x = x[:, np.newaxis] y = y[:, np.newaxis] model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "\n",
        "y_pred = model.predict(x)\n",
        "plt.scatter(x, y, s=10)\n",
        "plt.plot(x, y_pred, color='r')\n",
        "plt.show()\n",
        "```\n",
        "Scatter plot:\n",
        "\n",
        "The straight line is unable to capture the patterns in the data, as can be seen. This is an example of under-fitting.\n",
        "\n",
        "*  Let’s look at it from a technical standpoint, using measures like Root Mean Square Error (RMSE) and discrimination coefficient (R2). The RMSE indicates how well a regression model can predict the response variable’s value in absolute terms, whereas the R2 indicates how well a model can predict the response variable’s value in percentage terms.\n",
        "\n",
        "```\n",
        "import sklearn.metrics as metrics\n",
        "mse = metrics.mean_squared_error(x,y)\n",
        "rmse = np.sqrt(mse) \n",
        "r2 = metrics.r2_score(x,y)\n",
        "print('RMSE value:',rmse)\n",
        "print('R2 value:',r2)\n",
        "RMSE value: 93.47170875128153\n",
        "R2 value: -786.2378753237103\n",
        "```\n",
        "\n",
        "**Non-linear data – Polynomial Regression:**\n",
        "\n",
        "Because the weights associated with the features are still linear, this is still called a linear model. x2 (x square) is only a function. However, the curve we’re trying to fit is quadratic in nature.\n",
        "\n",
        "Let’s see visually the above concept for better understanding, a picture speaks louder and stronger than words,\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "polynomial_features1 = PolynomialFeatures(degree=2)\n",
        "x_poly1 = polynomial_features1.fit_transform(x)\n",
        "model1 = LinearRegression()\n",
        "model1.fit(x_poly1, y)\n",
        "y_poly_pred1 = model1.predict(x_poly1)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "rmse1 = np.sqrt(mean_squared_error(y,y_poly_pred1))\n",
        "r21 = r2_score(y,y_poly_pred1)\n",
        "print(rmse1)\n",
        "print(r21)\n",
        "49.66562739942289\n",
        "0.7307277801966172\n",
        "```\n",
        "\n",
        "The figure clearly shows that the quadratic curve can better match the data than the linear line.\n",
        "\n",
        "```\n",
        "import operator\n",
        "plt.scatter(x, y, s=10)\n",
        "# sort the values of x before line plot\n",
        "sort_axis = operator.itemgetter(0)\n",
        "sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
        "x, y_poly_pred1 = zip(*sorted_zip)\n",
        "plt.plot(x, y_poly_pred1, color='m')\n",
        "plt.show()\n",
        "quadratic curve\n",
        "polynomial_features2= PolynomialFeatures(degree=3)\n",
        "x_poly2 = polynomial_features2.fit_transform(x)\n",
        "model2 = LinearRegression()\n",
        "model2.fit(x_poly2, y)\n",
        "y_poly_pred2 = model2.predict(x_poly2)\n",
        "rmse2 = np.sqrt(mean_squared_error(y,y_poly_pred2))\n",
        "r22 = r2_score(y,y_poly_pred2)\n",
        "print(rmse2)\n",
        "print(r22)\n",
        "48.00085922331635\n",
        "0.7484769902353146\n",
        "plt.scatter(x, y, s=10)\n",
        "# sort the values of x before line plot\n",
        "sort_axis = operator.itemgetter(0)\n",
        "sorted_zip = sorted(zip(x,y_poly_pred2), key=sort_axis)\n",
        "x, y_poly_pred2 = zip(*sorted_zip)\n",
        "plt.plot(x, y_poly_pred2, color='m')\n",
        "plt.show()\n",
        "Scatter plot for Polynomial Regression\n",
        "polynomial_features3= PolynomialFeatures(degree=4)\n",
        "x_poly3 = polynomial_features3.fit_transform(x)\n",
        "model3 = LinearRegression()\n",
        "model3.fit(x_poly3, y)\n",
        "y_poly_pred3 = model3.predict(x_poly3)\n",
        "rmse3 = np.sqrt(mean_squared_error(y,y_poly_pred3))\n",
        "r23 = r2_score(y,y_poly_pred3)\n",
        "print(rmse3)\n",
        "print(r23)\n",
        "40.009589710152866\n",
        "0.8252537381840246\n",
        "plt.scatter(x, y, s=10)\n",
        "\n",
        "# sort the values of x before line plot\n",
        "\n",
        "sort_axis = operator.itemgetter(0)\n",
        "\n",
        "sorted_zip = sorted(zip(x,y_poly_pred3), key=sort_axis)\n",
        "\n",
        "x, y_poly_pred3 = zip(*sorted_zip)\n",
        "\n",
        "plt.plot(x, y_poly_pred3, color='m')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "Scatter plot:\n",
        "\n",
        "In comparison to the linear line, we can observe that RMSE has dropped and R2-score has increased.\n",
        "\n",
        "**Overfitting Vs Under-fitting:**\n",
        "\n",
        "*  We keep on increasing the degree, we will see the best result, but there comes the over-fitting problem, if we get r2 value for a particular value shows 100.\n",
        "\n",
        "*  When analyzing a dataset linearly, we encounter an under-fitting problem, which can be corrected using polynomial regression. However, when fine-tuning the degree parameter to the optimal value, we encounter an over-fitting problem, resulting in a 100 per cent r2 value. The conclusion is that we must avoid both overfitting and underfitting issues.\n",
        "\n",
        "Note: \n",
        "\n",
        "*  To avoid over-fitting, we can increase the number of training samples so that the algorithm does not learn the system’s noise and becomes more generalized.\n",
        "\n",
        "**Bias Vs Variance Tradeoff:**\n",
        "\n",
        "*  How do we pick the best model? To address this question, we must first comprehend the trade-off between bias and variance.\n",
        "\n",
        "*  The mistake caused by the model’s simple assumptions in fitting the data is referred to as bias. A high bias indicates that the model is unable to capture data patterns, resulting in under-fitting.\n",
        "\n",
        "*  The mistake caused by the complicated model trying to match the data is referred to as variance. When a model has a high variance, it passes over the majority of the data points, causing the data to overfit.\n",
        "\n",
        "*  From the above program, when degree is 1 which means in linear regression, it shows underfitting which means high bias and low variance. And when we get r2 value 100, which means low bias and high variance, which means overfitting\n",
        "\n",
        "*  As the model complexity grows, the bias reduces while the variance increases, and vice versa. A machine learning model should, in theory, have minimal variance and bias. However, having both is nearly impossible. As a result, a trade-off must be made in order to build a strong model that performs well on both train and unseen data.\n",
        "\n",
        "\n",
        "**Degree – how to find the right one?**\n",
        "\n",
        "We need to find the right degree of polynomial parameter, in order to avoid overfitting and underfitting problems,\n",
        "\n",
        "1. Forward selection: increase the degree parameter till you get the optimal result\n",
        "\n",
        "2. Backward selection: decrease degree parameter till you get optimal\n",
        "\n",
        "**Loss and Cost function – Polynomial Regression:**\n",
        "\n",
        "*  The Cost Function is a function that evaluates a Machine Learning model’s performance for a given set of data. The Cost Function is a single real number that calculates the difference between anticipated and expected values. Many people are confused by the differences between the Cost Function and the Loss Function.\n",
        "*  To put it another way, the Cost Function is the average of the n-sample error in the data, whereas the Loss Function is the error for individual data points. To put it another way, the Loss Function refers to a single training example, whereas the Cost Function refers to the complete training set.\n",
        "\n",
        "*  The Mean Squared Error may also be used as the Cost Function of Polynomial regression; however, the equation will vary somewhat.\n",
        "\n",
        "*  We now know that the Cost Function’s optimum value is 0 or a close approximation to 0. To get an optimal Cost Function, we may use Gradient Descent, which changes the weight and, as a result, reduces mistakes.\n",
        "\n",
        "**Gradient Descent – Polynomial Regression:**\n",
        "\n",
        "*  Gradient descent is a method of determining the values of a function’s parameters (coefficients) in order to minimize a cost function (cost). It may be used to decrease the Cost function (minimizing MSE value) and achieve the best fit line.\n",
        "\n",
        "*  The values of slope (m) and slope-intercept (b) will be set to 0 at the start of the function, and the learning rate (α) will be introduced. The learning rate (α) is set to an extremely low number, perhaps between 0.01 and 0.0001. The learning rate is a tuning parameter in an optimization algorithm that sets the step size at each iteration as it moves toward the cost function’s minimum. The partial derivative is then determined in terms of m for the cost function equation, as well as derivatives with regard to the b.\n",
        "\n",
        "\n",
        "*  Gradient indicates the steepest climb of the loss function, but the steepest fall is the inverse of the gradient, which is why the gradient is subtracted from the weights (m and b). The process of updating the values of m and b continues until the cost function achieves or approaches the ideal value of 0. The current values of m and b will be the best fit line’s optimal value.\n",
        "\n",
        "Practical application of Polynomial Regression\n",
        "We will start with importing the libraries,\n",
        "\n",
        "```\n",
        "#with dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('Position_Salaries.csv')\n",
        "dataset\n",
        "Segregating the dataset into dependent and independent features,\n",
        "\n",
        "X = dataset.iloc[:,1:2].values  \n",
        "y = dataset.iloc[:,2].values\n",
        "Then trying with linear regression,\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X,y)\n",
        "Visually linear regression can be seen,\n",
        "\n",
        "plt.scatter(X,y, color='red')\n",
        "plt.plot(X, lin_reg.predict(X),color='blue')\n",
        "plt.title(\"Truth or Bluff(Linear)\")\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n",
        " \n",
        "\n",
        "Plot for Linear Regression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_reg = PolynomialFeatures(degree=2)\n",
        "X_poly = poly_reg.fit_transform(X)\n",
        "lin_reg2 = LinearRegression()\n",
        "lin_reg2.fit(X_poly,y)\n",
        "```\n",
        "\n",
        "**Application of Polynomial Regression:**\n",
        "\n",
        "This equation is used to obtain the results in various experimental techniques. The independent and dependent variables have a well-defined connection. It’s used to figure out what isotopes are present in sediments. It’s utilized to look at the spread of various illnesses across a population. It’s utilized to research how synthesis is created.\n",
        "\n",
        "**Advantage of Polynomial Regression:**\n",
        "\n",
        "The best approximation of the connection between the dependent and independent variables is a polynomial. It can accommodate a wide range of functions. Polynomial is a type of curve that can accommodate a wide variety of curvatures.\n",
        "\n",
        "**Disadvantages of Polynomial Regression:**\n",
        "\n",
        "One or two outliers in the data might have a significant impact on the nonlinear analysis’ outcomes. These are overly reliant on outliers. Furthermore, there are fewer model validation methods for detecting outliers in nonlinear regression than there are for linear regression.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oyu67w45e-tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Ridge And Lasso Regression:**\n",
        "\n",
        "Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. If you’ve heard of them before, you must know that they work by penalizing the magnitude of coefficients of features and minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques. The key difference is in how they assign penalties to the coefficients:\n",
        "\n",
        "**Ridge Regression:**\n",
        "\n",
        "Performs L2 regularization, i.e., adds penalty equivalent to the square of the magnitude of coefficients\n",
        "Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
        "\n",
        "**Lasso Regression:**\n",
        "\n",
        "Performs L1 regularization, i.e., adds penalty equivalent to the absolute value of the magnitude of coefficients\n",
        "Minimization objective = LS Obj + α * (sum of the absolute value of coefficients)\n",
        "Here, LS Obj refers to the ‘least squares objective,’ i.e., the linear regression objective without regularization.\n",
        "\n",
        "If terms like ‘penalty’ and ‘regularization’ seem very unfamiliar to you, don’t worry; we’ll discuss these in more detail throughout this article. Before digging further into how they work, let’s try to understand why penalizing the magnitude of coefficients should work in the first place.\n",
        "\n",
        "**Why Penalize the Magnitude of Coefficients?**\n",
        "\n",
        "Let’s try to understand the impact of model complexity on the magnitude of coefficients. As an example, I have simulated a sine curve (between 60° and 300°) and added some random noise using the following code:\n",
        "\n",
        "Python Code:\n",
        "\n",
        "\n",
        "This resembles a sine curve but not exactly because of the noise. We’ll use this as an example to test different scenarios in this article. Let’s try to estimate the sine function using polynomial regression with powers of x from 1 to 15. Let’s add a column for each power upto 15 in our dataframe. This can be accomplished using the following code:\n",
        "\n",
        "for i in range(2,16):  #power of 1 is already there\n",
        "    colname = 'x_%d'%i      #new var will be x_power\n",
        "    data[colname] = data['x']**i\n",
        "print(data.head())\n",
        "The dataframe looks like this:\n",
        "polynomial regression\n",
        "\n",
        "Now that we have all the 15 powers, let’s make 15 different linear regression models, with each model containing variables with powers of x from 1 to the particular model number. For example, the feature set of model 8 will be – {x, x_2, x_3, …, x_8}.\n",
        "\n",
        "First, we’ll define a generic function that takes in the required maximum power of x as an input and returns a list containing – [ model RSS, intercept, coef_x, coef_x2, … upto entered power ]. Here RSS refers to the ‘Residual Sum of Squares,’ which is nothing but the sum of squares of errors between the predicted and actual values in the training data set and is known as the cost function or the loss function. The python code defining the function is:\n",
        "\n",
        "#Import Linear Regression model from scikit-learn.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "def linear_regression(data, power, models_to_plot):\n",
        "    #initialize predictors:\n",
        "    predictors=['x']\n",
        "    if power>=2:\n",
        "        predictors.extend(['x_%d'%i for i in range(2,power+1)])\n",
        "    \n",
        "    #Fit the model\n",
        "    linreg = LinearRegression(normalize=True)\n",
        "    linreg.fit(data[predictors],data['y'])\n",
        "    y_pred = linreg.predict(data[predictors])\n",
        "    \n",
        "    #Check if a plot is to be made for the entered power\n",
        "    if power in models_to_plot:\n",
        "        plt.subplot(models_to_plot[power])\n",
        "        plt.tight_layout()\n",
        "        plt.plot(data['x'],y_pred)\n",
        "        plt.plot(data['x'],data['y'],'.')\n",
        "        plt.title('Plot for power: %d'%power)\n",
        "    \n",
        "    #Return the result in pre-defined format\n",
        "    rss = sum((y_pred-data['y'])**2)\n",
        "    ret = [rss]\n",
        "    ret.extend([linreg.intercept_])\n",
        "    ret.extend(linreg.coef_)\n",
        "    return ret\n",
        "Note that this function will not plot the model fit for all the powers but will return the RSS and coefficient values for all the models. I’ll skip the details of the code for now to maintain brevity. I’ll be happy to discuss the same through the comments below if required.\n",
        "\n",
        "Now, we can make all 15 models and compare the results. For ease of analysis, we’ll store all the results in a Pandas dataframe and plot 6 models to get an idea of the trend. Consider the following code:\n",
        "\n",
        "#Initialize a dataframe to store the results:\n",
        "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
        "ind = ['model_pow_%d'%i for i in range(1,16)]\n",
        "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
        "\n",
        "#Define the powers for which a plot is required:\n",
        "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
        "\n",
        "#Iterate through all powers and assimilate results\n",
        "for i in range(1,16):\n",
        "    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)\n",
        "We would expect the models with increasing complexity to better fit the data and result in lower RSS values. This can be verified by looking at the plots generated for 6 models:\n",
        "\n",
        "linear regression, rss\n",
        "\n",
        "This clearly aligns with our initial understanding. As the model complexity increases, the models tend to fit even smaller deviations in the training data set. Though this leads to overfitting, let’s keep this issue aside for some time and come to our main objective, i.e., the impact on the magnitude of coefficients. This can be analyzed by looking at the data frame created above.\n",
        "\n",
        "Python Code:\n",
        "\n",
        "#Set the display format to be scientific for ease of analysis\n",
        "pd.options.display.float_format = '{:,.2g}'.format\n",
        "coef_matrix_simple\n",
        "The output looks like this:\n",
        "regression coefficient\n",
        "\n",
        "It is clearly evident that the size of coefficients increases exponentially with an increase in model complexity. I hope this gives some intuition into why putting a constraint on the magnitude of coefficients can be a good idea to reduce model complexity.\n",
        "\n",
        "Let’s try to understand this even better.\n",
        "\n",
        "\n",
        "What does a large coefficient signify? It means that we’re putting a lot of emphasis on that feature, i.e., the particular feature is a good predictor for the outcome. When it becomes too large, the algorithm starts modeling intricate relations to estimate the output and ends up overfitting the particular training data.\n",
        "\n",
        "I hope the concept is clear. Now, let’s understand ridge and lasso regression in detail and see how well they work for the same problem.\n",
        "\n",
        "**How Does Ridge Regression Work?**\n",
        "\n",
        "As mentioned before, ridge regression performs ‘L2 regularization‘, i.e., it adds a factor of the sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
        "\n",
        "**Objective = RSS + α * (sum of the square of coefficients)**\n",
        "\n",
        "Here, α (alpha) is the parameter that balances the amount of emphasis given to minimizing RSS vs minimizing the sum of squares of coefficients. α can take various values:\n",
        "\n",
        "α = 0:\n",
        "The objective becomes the same as simple linear regression.\n",
        "We’ll get the same coefficients as simple linear regression.\n",
        "α = ∞:\n",
        "The coefficients will be zero. Why? Because of infinite weightage on the square of coefficients, anything less than zero will make the objective infinite.\n",
        "0 < α < ∞:\n",
        "The magnitude of α will decide the weightage given to different parts of the objective.\n",
        "The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
        "I hope this gives some sense of how α would impact the magnitude of coefficients. One thing is for sure – any non-zero value would give values less than that of simple linear regression. By how much? We’ll find out soon. Leaving the mathematical details for later, let’s see ridge regression in action on the same problem as above.\n",
        "\n",
        "First, let’s define a generic function for ridge regression similar to the one defined for simple linear regression. The Python code is:\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "def ridge_regression(data, predictors, alpha, models_to_plot={}):\n",
        "    #Fit the model\n",
        "    ridgereg = Ridge(alpha=alpha,normalize=True)\n",
        "    ridgereg.fit(data[predictors],data['y'])\n",
        "    y_pred = ridgereg.predict(data[predictors])\n",
        "    \n",
        "    #Check if a plot is to be made for the entered alpha\n",
        "    if alpha in models_to_plot:\n",
        "        plt.subplot(models_to_plot[alpha])\n",
        "        plt.tight_layout()\n",
        "        plt.plot(data['x'],y_pred)\n",
        "        plt.plot(data['x'],data['y'],'.')\n",
        "        plt.title('Plot for alpha: %.3g'%alpha)\n",
        "    \n",
        "    #Return the result in pre-defined format\n",
        "    rss = sum((y_pred-data['y'])**2)\n",
        "    ret = [rss]\n",
        "    ret.extend([ridgereg.intercept_])\n",
        "    ret.extend(ridgereg.coef_)\n",
        "    return ret\n",
        "Note the ‘Ridge’ function used here. It takes ‘alpha’ as a parameter on initialization. Also, keep in mind that normalizing the inputs is generally a good idea in every type of regression and should be used in the case of ridge regression as well.\n",
        "\n",
        "Now, let’s analyze the result of Ridge regression for 10 different values of α ranging from 1e-15 to 20. These values have been chosen so that we can easily analyze the trend with changes in values of α. These would, however, differ from case to case.\n",
        "\n",
        "Note that each of these 10 models will contain all the 15 variables, and only the value of alpha would differ. This differs from the simple linear regression case, where each model had a subset of features.\n",
        "\n",
        "Python Code:\n",
        "\n",
        "#Initialize predictors to be set of 15 powers of x\n",
        "predictors=['x']\n",
        "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
        "\n",
        "#Set the different values of alpha to be tested\n",
        "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
        "\n",
        "#Initialize the dataframe for storing coefficients.\n",
        "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
        "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
        "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
        "\n",
        "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
        "for i in range(10):\n",
        "    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)\n",
        "This would generate the following plot:\n",
        "ridge regression, l2 regularization\n",
        "\n",
        "Here we can clearly observe that as the value of alpha increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (e.g., alpha = 5). Thus alpha should be chosen wisely. A widely accepted technique is cross-validation, i.e., the value of alpha is iterated over a range of values, and the one giving a higher cross-validation score is chosen.\n",
        "\n",
        "Let’s have a look at the value of coefficients in the above models:\n",
        "\n",
        "Python Code:\n",
        "\n",
        "#Set the display format to be scientific for ease of analysis\n",
        "pd.options.display.float_format = '{:,.2g}'.format\n",
        "coef_matrix_ridge\n",
        "The table looks like:\n",
        "ridge, python\n",
        "\n",
        "This straight away gives us the following inferences:\n",
        "\n",
        "The RSS increases with an increase in alpha.\n",
        "An alpha value as small as 1e-15 gives us a significant reduction in the magnitude of coefficients. How? Compare the coefficients in the first row of this table to the last row of the simple linear regression table.\n",
        "High alpha values can lead to significant underfitting. Note the rapid increase in RSS for values of alpha greater than 1\n",
        "Though the coefficients are really small, they are NOT zero.\n",
        "The first 3 are very intuitive. But #4 is also a crucial observation. Let’s reconfirm the same by determining the number of zeros in each row of the coefficients data set:\n",
        "\n",
        "Python Code:\n",
        "\n",
        "coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)\n",
        "Output:\n",
        "ridge regression coefficients\n",
        "\n",
        "This confirms that all 15 coefficients are greater than zero in magnitude (can be +ve or -ve). Remember this observation and have a look again until it’s clear. This will play an important role later while comparing ridge with lasso regression.\n",
        "\n",
        "**How Does Lasso Regression Work?**\n",
        "\n",
        "LASSO stands for Least Absolute Shrinkage and Selection Operator. I know it doesn’t give much of an idea, but there are 2 keywords here – ‘absolute‘ and ‘selection. ‘\n",
        "\n",
        "Let’s consider the former first and worry about the latter later.\n",
        "\n",
        "Lasso regression performs L1 regularization, i.e., it adds a factor of the sum of the absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
        "\n",
        "**Objective = RSS + α * (sum of the absolute value of coefficients)**\n",
        "\n",
        "Here, α (alpha) works similar to that of the ridge and provides a trade-off between balancing RSS and the magnitude of coefficients. Like that of the ridge, α can take various values. Let’s iterate it here briefly:\n",
        "\n",
        "α = 0: Same coefficients as simple linear regression\n",
        "α = ∞: All coefficients zero (same logic as before)\n",
        "0 < α < ∞: coefficients between 0 and that of simple linear regression\n",
        "Yes, its appearing to be very similar to Ridge till now. But hang on with me, and you’ll know the difference by the time we finish. Like before, let’s run lasso regression on the same problem as above. First, we’ll define a generic function:\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
        "    #Fit the model\n",
        "    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
        "    lassoreg.fit(data[predictors],data['y'])\n",
        "    y_pred = lassoreg.predict(data[predictors])\n",
        "    \n",
        "    #Check if a plot is to be made for the entered alpha\n",
        "    if alpha in models_to_plot:\n",
        "        plt.subplot(models_to_plot[alpha])\n",
        "        plt.tight_layout()\n",
        "        plt.plot(data['x'],y_pred)\n",
        "        plt.plot(data['x'],data['y'],'.')\n",
        "        plt.title('Plot for alpha: %.3g'%alpha)\n",
        "    \n",
        "    #Return the result in pre-defined format\n",
        "    rss = sum((y_pred-data['y'])**2)\n",
        "    ret = [rss]\n",
        "    ret.extend([lassoreg.intercept_])\n",
        "    ret.extend(lassoreg.coef_)\n",
        "    return ret\n",
        "Notice the additional parameters defined in the Lasso function – ‘max_iter. ‘ This is the maximum number of iterations for which we want the model to run if it doesn’t converge before. This exists for Ridge as well, but setting this to a higher than default value was required in this case. Why? I’ll come to this in the next section.\n",
        "\n",
        "Let’s check the output for 10 different values of alpha using the following code:\n",
        "\n",
        "#Initialize predictors to all 15 powers of x\n",
        "predictors=['x']\n",
        "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
        "\n",
        "#Define the alpha values to test\n",
        "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
        "\n",
        "#Initialize the dataframe to store coefficients\n",
        "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
        "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
        "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
        "\n",
        "#Define the models to plot\n",
        "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
        "\n",
        "#Iterate over the 10 alpha values:\n",
        "for i in range(10):\n",
        "    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)\n",
        "This gives us the following plots:\n",
        "lasso regression, l1 regularization\n",
        "\n",
        "This again tells us that the model complexity decreases with an increase in the values of alpha. But notice the straight line at alpha=1. Appears a bit strange to me. \n",
        "\n",
        "\n",
        "Apart from the expected inference of higher RSS for higher alphas, we can see the following:\n",
        "\n",
        "For the same values of alpha, the coefficients of lasso regression are much smaller than that of ridge regression (compare row 1 of the 2 tables).\n",
        "For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression.\n",
        "Many of the coefficients are zero, even for very small values of alpha.\n",
        "Inferences #1 and 2 might not always generalize but will hold for many cases. The real difference from the ridge is coming out in the last inference. Let’s check the number of coefficients that are zero in each model using the following code:\n",
        "\n",
        "coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)\n",
        "Output:\n",
        "lasso regression coefficients\n",
        "\n",
        "We can observe that even for a small value of alpha, a significant number of coefficients are zero. This also explains the horizontal line fit for alpha=1 in the lasso plots; it’s just a baseline model! This phenomenon of most of the coefficients being zero is called ‘sparsity. ‘ Although lasso performs feature selection, this level of sparsity is achieved in special cases only, which we’ll discuss towards the end.\n",
        "\n",
        "This has some really interesting implications on the use cases of lasso regression as compared to that of ridge regression. But before coming to the final comparison, let’s take a bird’s eye view of the mathematics behind why coefficients are zero in the case of lasso but not ridge.\n",
        "\n",
        "\n",
        "\n",
        "**Ridge Regression:**\n",
        "\n",
        "The objective function (also called the cost) to be minimized is the RSS plus the sum of squares of the magnitude of weights. This can be depicted mathematically as:\n",
        "\n",
        "ridge regression, cost function\n",
        "\n",
        "In this case, the gradient would be:\n",
        "\n",
        "**ridge regression, gradient\n",
        "\n",
        "Again in the regularization part of a gradient, only wj remains, and all others would become zero. The corresponding update rule is:\n",
        "\n",
        "**ridge regression, regularization**\n",
        "\n",
        "Here we can see that the second part of the RHS is the same as that of simple linear regression. Thus, ridge regression is equivalent to reducing the weight by a factor of (1-2λη) first and then applying the same update rule as simple linear regression. I hope this explains why the coefficients get reduced to small numbers but never become zero.\n",
        "\n",
        "Note that the criteria for convergence, in this case, remains similar to simple linear regression, i.e., checking the value of gradients. Let’s discuss Lasso regression now.\n",
        "\n",
        "Lasso Regression\n",
        "The objective function (also called the cost) to be minimized is the RSS plus the sum of the absolute value of the magnitude of weights. This can be depicted mathematically as:\n",
        "\n",
        "lasso regression, cost function\n",
        "\n",
        "In this case, the gradient is not defined as the absolute function is not differentiable at x=0. This can be illustrated as:\n",
        "\n",
        "\n",
        "\n",
        "**Comparison Between Ridge Regression and Lasso Regression:**\n",
        "\n",
        "Now that we have a fair idea of how ridge and lasso regression work, let’s try to consolidate our understanding by comparing them and appreciating their specific use cases. I will also compare them with some alternate approaches. Let’s analyze these under three buckets:\n",
        "\n",
        "**Key Difference:**\n",
        "\n",
        "Ridge: \n",
        "\n",
        "It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.\n",
        "\n",
        "Lasso:\n",
        "\n",
        " Along with shrinking coefficients, the lasso also performs feature selection. (Remember the ‘selection‘ in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.\n",
        "Traditionally, techniques like stepwise regression were used to perform feature selection and make parsimonious models. But with advancements in Machine-Learning, ridge and lasso regressions provide very good alternatives as they give much better output, require fewer tuning parameters, and can be automated to a large extent.\n",
        "\n",
        "**Typical Use Cases:**\n",
        "\n",
        "Ridge: It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in the case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n",
        "Lasso: Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modeling cases where the #features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can be ignored.\n",
        "It’s not hard to see why the stepwise selection techniques become practically cumbersome to implement in high-dimensionality cases. Thus, the lasso provides a significant advantage.\n",
        "\n",
        "**Presence of Highly Correlated Features:**\n",
        "\n",
        "Ridge:\n",
        "\n",
        " It generally works well even in the presence of highly correlated features, as it will include all of them in the model. Still, the coefficients will be distributed among them depending on the correlation.\n",
        "\n",
        "Lasso: \n",
        "\n",
        "It arbitrarily selects any feature among the highly correlated ones and reduces the coefficients of the rest to zero. Also, the chosen variable changes randomly with changes in model parameters. This generally doesn’t work that well as compared to ridge regression.\n",
        "\n",
        "This disadvantage of the lasso can be observed in the example we discussed above. Since we used a polynomial regression, the variables were highly correlated. (Not sure why? Check the output of data.corr() ). Thus, we saw that even small values of alpha were giving significant sparsity (i.e., high #coefficients as zero).\n",
        "\n",
        "Along with Ridge and Lasso, Elastic Net is another useful technique that combines both L1 and L2 regularization. It can be used to balance out the pros and cons of ridge and lasso regression. I encourage you to explore it further.\n",
        "\n",
        "\n",
        "\n",
        "##**Frequently Asked Questions:**\n",
        "\n",
        "**Q1. What is the difference between ridge and lasso regression?**\n",
        "\n",
        "Ridge and lasso regression both address multicollinearity in regression models but are different in the type of penalty used. Ridge regression (L2 regularization) shrinks coefficients towards zero, whereas lasso regression (L1 regularization) can force some coefficients to be exactly 0, making it suitable for feature selection.\n",
        "\n",
        "**Q2. How can a data scientist use mean squared error as a metric to evaluate the performance of Ridge and Lasso regression models in Python?**\n",
        "\n",
        "Mean squared error (MSE) is used to measure the performance of Ridge and Lasso regression models in Python. The goal is to minimize the MSE between predicted and actual values, and the model’s performance is compared by calculating the MSE using scikit-learn’s mean_squared_error function. A lower MSE indicates better performance, but the choice of hyperparameters can affect the results.\n",
        "\n",
        "**Q3. What are the benefits and limitations of using ridge and lasso regression?**\n",
        "\n",
        "Ridge and Lasso regression offer benefits in regression analysis, including addressing multicollinearity, regularization, feature selection, and flexibility, which make them popular techniques for various regression problems.\n",
        "\n",
        "In addition to the benefits mentioned above, Ridge and Lasso regression also have some limitations, such as the need to choose appropriate hyperparameters and the potential for bias towards a specific set of predictors. Despite these limitations, Ridge and Lasso regression are widely used in many practical applications and can provide valuable insights for regression problems.\n",
        "\n",
        "**Q4. when to use ridge and lasso regression?**\n",
        "\n",
        "Ridge regression and Lasso regression are both regularization techniques used to prevent overfitting in linear regression models.\n",
        "\n",
        "Ridge regression adds a penalty term to the least squares objective function, which is proportional to the square of the magnitude of the coefficients. This penalty term shrinks the coefficients towards zero, but does not eliminate any of them completely. It can be useful when dealing with multicollinearity, where there are high correlations among the predictor variables.\n",
        "\n",
        "Lasso regression, on the other hand, also adds a penalty term to the objective function, but it is proportional to the absolute value of the coefficients. This penalty term can shrink some of the coefficients to exactly zero, effectively eliminating some of the predictor variables from the model. Lasso regression can be useful for feature selection, where we want to identify the most important predictor variables in the model.\n",
        "\n",
        "**So, to decide between Ridge and Lasso regression, we need to consider the following factors:**\n",
        "\n",
        "If we have a large number of predictor variables and suspect that many of them may not be important, we can use Lasso regression for feature selection.\n",
        "If we have a smaller number of predictor variables, but they are highly correlated, we can use Ridge regression to reduce multicollinearity.\n",
        "If we are unsure which to use, we can try both and compare their performance using cross-validation.\n",
        "Overall, Ridge and Lasso regression are both useful techniques for regularization in linear regression models, and the choice between them depends on the specific problem and the characteristics of the dataset.\n",
        "\n",
        "\n",
        "**Q5. cross validation for learning parameter alpha with real example?**\n",
        "\n",
        "Sure, here's an example of using cross-validation to tune the regularization parameter alpha for Ridge regression:\n",
        "\n",
        "Let's say we have a dataset with features X and target variable y, and we want to fit a Ridge regression model to it. The Ridge regression model has a regularization parameter alpha that controls the strength of the regularization penalty.\n",
        "\n",
        "To find the optimal value of alpha, we can use cross-validation. Here's the \n",
        "\n",
        "**step-by-step process:**\n",
        "\n",
        "```\n",
        "\n",
        "#Split the data into training and test sets#\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "#Define the range of alpha values to test#\n",
        "\n",
        "import numpy as np\n",
        "alphas = np.logspace(-3, 3, 7)\n",
        "\n",
        "This creates an array of 7 alpha values ranging from 0.001 to 1000.\n",
        "\n",
        "#Perform cross-validation on the training set#\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "mse_scores = []\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    mse = -1 * cross_val_score(ridge, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    mse_scores.append(np.mean(mse))\n",
        "\n",
        "Here, we're using a Ridge regression model with each value of alpha, and \n",
        "\n",
        "computing the negative mean squared error (MSE) for each fold of the 5-fold \n",
        "\n",
        "cross-validation.\n",
        "\n",
        "#Select the best value of alpha#\n",
        "\n",
        "best_alpha = alphas[np.argmin(mse_scores)]\n",
        "\n",
        "Here, we're using NumPy's argmin function to find the index of the alpha value \n",
        "\n",
        "with the lowest MSE, and then selecting that value from the original alpha \n",
        "\n",
        "array.\n",
        "\n",
        "#Evaluate the model on the test set#\n",
        "\n",
        "ridge = Ridge(alpha=best_alpha)\n",
        "ridge.fit(X_train, y_train)\n",
        "test_mse = mean_squared_error(y_test, ridge.predict(X_test))\n",
        "```\n",
        "\n",
        "Here, we're fitting a Ridge regression model with the best value of alpha on the entire training set, and then computing the MSE on the test set using the mean_squared_error function from scikit-learn.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L2DL8NFah9fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Elastic Net Regression:**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Elastic net is a combination of the two most popular regularized variants of linear regression: ridge and lasso.  Ridge utilizes an L2 penalty and lasso uses an L1 penalty. With elastic net, you don't have to choose between these two models, because elastic net uses both the L2 and the L1 penalty! In practice, you will almost always want to use elastic net over ridge or lasso, and in this article you will learn everything you need to know to do so, successfully.....\n",
        "\n",
        "*  You have probably heard about linear regression. Most likely you have also heard\n",
        "about ridge and lasso. Maybe you have even read\n",
        "some articles about ridge and lasso.\n",
        "Ridge and lasso are the two most popular variations of\n",
        "linear regression which try to make it a bit more robust. Nowadays it is actually very uncommon\n",
        "to use regular linear regression, and not one of its variations like ridge or lasso.\n",
        "\n",
        "\n",
        "*  In previous articles we have seen how ridge and lasso\n",
        "operate, what their differences, as well as strengths and weaknesses are,\n",
        "and how you can implement them in practice.\n",
        "\n",
        "*  But what should you use? Ridge or lasso?\n",
        "The good news is that you don’t have to choose!\n",
        "With elastic net, you can use both the ridge penalty as well as the lasso penalty at once.\n",
        "\n",
        "*  And in this article, you will learn how!This article is the third article in a \n",
        "series where we take a deep dive into ridge and lasso regression.\n",
        "Let’s start!   Prerequisites  This article is a direct follow up to the articles\n",
        "about ridge and lasso,\n",
        "so ideally you should read the articles about ridge and lasso before reading this article.\n",
        "\n",
        "\n",
        "\n",
        "*  Elastic net is based on ridge and lasso, so it’s important to understand\n",
        "those models first.\n",
        "With that being said, let’s take a look at elastic net regression!   The Problem  So what is wrong with linear regression? Why do we need more machine learning algorithms\n",
        "that do the same thing? And why are there two of them? We’ve explored this question in the\n",
        "articles about ridge and lasso.\n",
        "\n",
        "\n",
        "\n",
        "*  linear regression model overfitting and we noticed that the main cause of overfitting were\n",
        "large model parameters.\n",
        "After discovering this insight, we developed a new loss function that penalizes large model parameters\n",
        "by adding a penalty term to our mean squared error.\n",
        "\n",
        "\n",
        "*  In this case we have ridge regression if L1-ratio = 0 and lasso regression if L1-ratio = 1.\n",
        "In most cases, unless you already have some information about the importance\n",
        "of your features, you should use elastic net instead of lasso or ridge.\n",
        "You can then use cross-validation to determine the best ratio between L1 and L2 penalty strength.\n",
        "*  Now let’s look at how we determine the optimal model parameters $\\boldsymbol{\\theta}$ for our elastic net model.   Solving Elastic Net  If L1-ratio = 0, we have ridge regression. This means that we can treat our model\n",
        "as a ridge regression model, and solve it in the same ways we would solve ridge regression.\n",
        "*  Namely, we can use the normal equation for ridge regression to solve our model directly,\n",
        "or we can use gradient descent to solve it iteratively.If L1-ratio = 1, we have lasso regression. Then we can solve it with the same ways we would use to solve lasso regression.\n",
        "*  Since our model contains absolute values, we can’t construct a normal equation,\n",
        "and neither can we use (regular) gradient descent. Instead,\n",
        "we can use an adaptation of gradient descent like subgradient descent or coordinate descent.If we are using both the L1 and the L2-penalty, then we also have absolute values,\n",
        "*  so we can use the same techniques as the ones we would use for lasso regression,\n",
        "like subgradient descent or coordinate descent.    Implementing Elastic Net Regression  If you’re interested in implementing elastic net from scratch,\n",
        "then I recommend that you take a look at the articles about subgradient descent or coordinate descent,\n",
        "where we do exactly that! In this article, we will use scikit-learn to help us out.\n",
        "Scikit-learn provides a ElasticNet-class, which implements coordinate descent under the hood.\n",
        "We can use it like this:\n",
        "\n",
        "```\n",
        "Copy elastic_pipeline = make_pipeline(StandardScaler(),ElasticNet(alpha=1, l1_ratio=0.1)) \n",
        "\n",
        "elastic_pipeline.fit(X_train, y_train)\n",
        "print(elastic_pipeline[1].intercept_, elastic_pipeline[1].coef_)\n",
        " # output: 41.0 [-1.2127174]Just like with lasso,\n",
        "```\n",
        "*  we can also use scikit-learn’s SGDRegressor-class, which uses truncated gradients instead of regular ones.\n",
        "Here’s the code:\n",
        "```\n",
        "Copy elastic_sgd_pipeline = make_pipeline(StandardScaler(), SGDRegressor(alpha=1, l1_ratio=0.1, penalty = \"elasticnet\"))                           elastic_sgd_pipeline.fit(X_train, y_train) \n",
        " print(elastic_sgd_pipeline[1].intercept_, elastic_sgd_pipeline[1].coef_)\n",
        "  # output: [40.69570804] [-1.21309447]\n",
        "```  \n",
        "  Cool! In practice, you should probably stick to ElasticNet instead of SGDRegressor since\n",
        "coordinate descent converges more quickly than the truncated SGD in this scenario.*Coordinate descent for lasso in particular is extremely efficient. \n",
        "\n",
        "* The article about coordinate descent\n",
        "goes into more depth as to why this is, but in general coordinate descent is the preferred way to train lasso or elastic net models.Since we’re using regularized models like lasso or elastic net it is important to first standardize our data before feeding it into our regularized model!\n",
        "*  If you’re interested in what happens when we don’t standardize our data, check out When You Should Standardize Your Data.\n",
        "There you will learn all about standardization as well as pipelines in scikit-learn, which is what we’ve\n",
        "used in the above code to make our lives a bit easier.   Parameter Sparsity Testing for Elastic Net  The most important property of lasso is that lasso produces sparse model weights,\n",
        "meaning weights can be set all the way to 0.\n",
        "\n",
        "*  Whenever you are presented with an implementation\n",
        "of lasso (or any model that incorporates an L1-penalty, like elastic net),\n",
        "you should verify that this property actually holds.\n",
        "The easiest way to do so is to generate a randomized dataset, fit the model on it,\n",
        "and see whether or not all of the parameters are zeroed-out. \n",
        "\n",
        "Here it goes:\n",
        "```\n",
        "Copy elastic_rand_pipeline = make_pipeline(StandardScaler(),ElasticNet(alpha=1, l1_ratio=0.1)) elastic_rand_pipeline.fit(X_rand, y_rand)\n",
        " print(elastic_rand_pipeline[1].intercept_, elastic_rand_pipeline[1].coef_)\n",
        " # output: # 0.4881255425051216 [-0.  0. -0. -0. -0. -0. -0. -0. -0.  0. -0.  0. -0. -0.  0.  0. -0. -0. #  -0.  0. -0.  0. -0. -0.  0. -0.  0.  0.  0. -0. -0.  0. -0. -0.  0.  0. # -0.  0. -0. -0.  0.  0.  0.  0. -0.  0.  0.  0. -0. -0.]\n",
        "```\n",
        "\n",
        "*  Nice, the weights are all zeroed out!\n",
        "We can perform the same test for SGDRegressor:\n",
        "\n",
        "```\n",
        "Copy elastic_sgd_rand_pipeline = make_pipeline(StandardScaler(), SGDRegressor(alpha=1, l1_ratio=0.1, penalty = \"elasticnet\"))                           elastic_sgd_rand_pipeline.fit(X_rand, y_rand) \n",
        " print(elastic_sgd_rand_pipeline[1].intercept_, elastic_sgd_rand_pipeline[1].coef_)\n",
        "  # output: # [0.46150165] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. #  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. #  0. 0.]\n",
        "  ```\n",
        "*  Nice!   Finding the optimal value for $\\alpha$ and the L1-ratio  Here, we can use the power of cross-validation to compute the most optimal\n",
        "parameters for our model. Scikit-learn even provides a special class for this\n",
        "called ElasticNetCV. It takes in an array of $\\alpha$-values to compare and select\n",
        "the best of. If no array of $\\alpha$-values is provided, scikit-learn will automatically\n",
        "determine the optimal value of $\\alpha$.\n",
        "\n",
        " We can use it like so:\n",
        " ```\n",
        " Copy elastic_cv_pipeline = make_pipeline(StandardScaler(),ElasticNetCV(l1_ratio=0.1)) elastic_cv_pipeline.fit(X_train, y_train) \n",
        " print(elastic_cv_pipeline[1].alpha_)\n",
        "  # output: 0.6385\n",
        "  ```\n",
        "*  that’s nice, but how can you find an optimal value for the L1-ratio?\n",
        "ElasticNetCV only determines the optimal value for $\\alpha$, so if we want to\n",
        "determine the optimal value for the L1-ratio as well, we’ll have to do an additional round\n",
        "of cross-validation. For this, we can use techniques such as grid or random search,\n",
        "which you can learn more about by reading the article Grid and Random Search Explained, Step by Step.\n",
        "\n",
        "\n",
        "##**Questions regarding elastic net regression:**\n",
        "\n",
        "**Q1. Advantages and disadvantages of elastic net regression  instead of using lasso or ridge individually?**\n",
        "\n",
        "Elastic Net Regression is a regularization technique that combines both Lasso and Ridge regression. Here are some advantages and disadvantages of using Elastic Net Regression instead of Lasso or Ridge individually:\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "*  It overcomes some of the limitations of Lasso and Ridge regression. Lasso may not work well when there are correlated predictors, while Ridge may not perform well when there are many predictors. Elastic Net Regression provides a balance between these two methods and is better suited for high-dimensional datasets.\n",
        "\n",
        "*  It can handle multiple predictors simultaneously and can provide a more accurate model than using Lasso or Ridge individually.\n",
        "\n",
        "*  Elastic Net Regression is less sensitive to outliers compared to other regression methods.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "*  Elastic Net Regression requires tuning of the hyperparameter alpha which controls the balance between the Lasso and Ridge penalties. Selecting the optimal value of alpha requires trial and error and can be time-consuming.\n",
        "\n",
        "*  Elastic Net Regression may not work well when there are highly correlated predictors. In such cases, one of the methods (Lasso or Ridge) may be preferred over Elastic Net.\n",
        "\n",
        "*  It can be computationally expensive when dealing with large datasets.\n",
        "\n",
        "In summary, Elastic Net Regression is a powerful regularization technique that combines the advantages of both Lasso and Ridge regression, providing a more flexible approach to model selection. However, it requires careful tuning of the hyperparameter alpha and may not be suitable for all datasets.\n",
        "\n",
        "**Q2. real example where we can use elastic instead of lasso or ridge?**\n",
        "\n",
        "Elastic Net regression is generally used in cases where there are a large number of predictor variables and a limited number of observations. It combines the advantages of both Lasso and Ridge regression by adding a linear combination of L1 and L2 regularization terms to the objective function.\n",
        "\n",
        "*  A real-world example where Elastic Net regression could be useful is in predicting housing prices. In this case, the dataset may contain a large number of features, such as the number of rooms, size of the property, location, and other amenities. Elastic Net regression can help in reducing the number of variables by selecting only the most important ones, while still allowing some variables that may have a weak effect on the target variable to be included in the model.\n",
        "\n",
        "*  Furthermore, Elastic Net regression can help in dealing with multicollinearity, a common problem in regression analysis where the predictor variables are highly correlated with each other. This occurs frequently in housing price prediction, where variables such as the size of the property and number of rooms may be highly correlated. Elastic Net regression can help in identifying and selecting the most relevant variables, thus reducing the risk of overfitting.\n",
        "\n",
        "*  However, a disadvantage of Elastic Net regression is that it requires more computational resources than Lasso or Ridge regression due to the added regularization term. Additionally, determining the optimal values of the regularization parameters can be challenging and may require cross-validation techniques to avoid overfitting."
      ],
      "metadata": {
        "id": "hwwf6nAU8BGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Support Vector Regression:**\n",
        "\n",
        "Support Vector Regression (SVR) is a supervised learning algorithm used for regression tasks. It uses the same principles as the Support Vector Machine (SVM) for classification tasks. In SVR, the goal is to find a line or hyperplane that best fits the data by minimizing the distance between the predicted and actual values.\n",
        "\n",
        "Let's take an example of a dataset containing information about the salary of employees based on their years of experience. We will use SVR to predict the salary of an employee based on their years of experience.\n",
        "\n",
        "```\n",
        "First, let's import the necessary libraries:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "Next, let's load the dataset using pandas:\n",
        "\n",
        "\n",
        "data = pd.read_csv('salary_dataset.csv')\n",
        "\n",
        "The dataset contains two columns, 'YearsExperience' and 'Salary'.\n",
        "We will use 'YearsExperience' as the input feature and 'Salary' as the target feature.\n",
        "\n",
        "X = data['YearsExperience'].values.reshape(-1,1)\n",
        "y = data['Salary'].values\n",
        "\n",
        "Before training the SVR model, we need to preprocess the data by scaling it using the StandardScaler:\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "Next, we will split the data into training and testing sets:\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "Now, we will train the SVR model:\n",
        "\n",
        "regressor = SVR(kernel='rbf')\n",
        "regressor.fit(X_train, y_train.ravel())\n",
        "\n",
        "We have used the RBF (Radial Basis Function) kernel in this example. \n",
        "The RBF kernel is commonly used in SVR.\n",
        "\n",
        "Finally, we will make predictions on the testing set and evaluate the\n",
        "performance of the model using the R-squared metric:\n",
        "\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "print('R-squared:', r2_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "The R-squared metric measures the goodness of fit of the model. It represents the proportion of variance in the target variable that can be explained by the input feature. A higher R-squared value indicates a better fit.\n",
        "\n",
        "Overall, SVR can be a useful regression algorithm in cases where linear regression fails to capture the complexity of the data, especially in cases where the data is non-linear.\n",
        "\n",
        "\n",
        "##**Questions related to SVR:**\n",
        "\n",
        "**Q1. why we have to know support vector regression even though there are many algorithms?**\n",
        "\n",
        "*  Support Vector Regression (SVR) is a powerful algorithm that is widely used in machine learning for its ability to handle both linear and non-linear data. Unlike many other regression algorithms, SVR can also handle outliers effectively. Therefore, knowing how to use SVR can be beneficial in a variety of scenarios where other regression algorithms may not perform well.\n",
        "\n",
        "*  For example, SVR can be useful in predicting stock prices, where the data is often non-linear and can have outliers. It can also be used in predicting real estate prices, where there may be non-linear relationships between features such as location, property size, and age. In both cases, using SVR can lead to more accurate predictions compared to other regression algorithms.\n",
        "\n",
        "*  Additionally, SVR can be used in situations where the number of features is very high compared to the number of data points. In such cases, traditional regression algorithms may suffer from overfitting or high variance, whereas SVR can provide a more stable and reliable solution.\n",
        "\n",
        "*  Overall, knowing how to use SVR can be beneficial in a wide range of scenarios, making it a valuable tool to have in your machine learning toolkit.\n",
        "\n",
        "\n",
        "**Q2. explain an example that uses svr instead of other regression techniques?**\n",
        "\n",
        "*  Support Vector Regression (SVR) is particularly useful when dealing with nonlinear and complex datasets. It is used when there is a non-linear relationship between the independent and dependent variables, and the data is not normally distributed.\n",
        "\n",
        "*  One example where SVR could be used is in predicting house prices. In this case, the independent variables include the size of the house, the number of bedrooms, the location, and other features of the property. The dependent variable is the price of the house. In this scenario, the relationship between the independent and dependent variables may not be linear, and SVR could be used to model this relationship.\n",
        "\n",
        "*  SVR is advantageous in this scenario because it can handle non-linear relationships and outliers well, while also providing robustness against noise in the data. Additionally, it is a powerful algorithm that can provide high accuracy in prediction.\n",
        "\n",
        "*  On the other hand, other regression techniques such as linear regression or polynomial regression may not perform as well as SVR in this scenario, as they assume a linear relationship between the independent and dependent variables, and may not be able to handle non-linear relationships or outliers effectively.\n",
        "\n",
        "*  Overall, SVR can be a powerful tool in cases where non-linear relationships exist between the independent and dependent variables, making it a valuable alternative to other regression techniques.\n"
      ],
      "metadata": {
        "id": "hPuKkvY2kT_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Decision Tree Regressor:**\n",
        "\n",
        "Decision Tree Regressor is a popular supervised learning algorithm used for solving regression problems. It works by recursively partitioning the data into subsets based on the values of different features, and then making a decision based on the target variable.\n",
        "\n",
        "**Here's how the algorithm works:**\n",
        "\n",
        "*  The algorithm starts by selecting the best feature to split the data based on a criterion such as Information Gain or Gini Index. This feature is the one that provides the most information gain or the lowest impurity.\n",
        "\n",
        "*  The dataset is then split into two subsets based on the value of the selected feature. The split is chosen such that the two resulting subsets have the most distinct target variable values possible.\n",
        "\n",
        "*  The algorithm continues this process recursively for each subset, selecting the next best feature to split the data and splitting the subset again.\n",
        "\n",
        "*  This process is continued until a stopping criterion is met, such as a maximum depth of the tree or a minimum number of samples required to split a node.\n",
        "\n",
        "*  Once the tree is built, new data points can be classified by traversing the tree from the root node to a leaf node. At each node, the decision is made based on the value of the corresponding feature until a leaf node is reached, which corresponds to a predicted target value.\n",
        "\n",
        "*  Decision Tree Regressor is an intuitive algorithm that is easy to understand and interpret. It is also robust to outliers and can handle non-linear relationships between features and the target variable.\n",
        "\n",
        "Here's an example of using Decision Tree Regressor on a dataset in Python:\n",
        "```\n",
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Loading the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2)\n",
        "\n",
        "# Initializing the Decision Tree Regressor model with max depth of 3\n",
        "model = DecisionTreeRegressor(max_depth=3)\n",
        "\n",
        "# Fitting the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating the model performance using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error:', mse)\n",
        "```\n",
        "\n",
        "In this example, we are loading a dataset and splitting it into training and testing sets. We then initialize a Decision Tree Regressor model with a maximum depth of 3, fit it to the training data, and make predictions on the testing data. Finally, we evaluate the performance of the model using mean squared error.\n",
        "\n",
        "\n",
        "##**Questions related to Decision Tree Regressor:**\n",
        "\n",
        "**Q1. why we have to learn about decision tree regressor when there are many like svr , linear ridge and lasso?**\n",
        "\n",
        "*  Decision tree regressor is a non-parametric regression algorithm that can be used to model complex, nonlinear relationships between features and targets. It is a versatile algorithm that can handle both continuous and categorical data, making it useful in a wide range of applications.\n",
        "\n",
        "*  One of the main advantages of decision tree regressor is that it can capture nonlinear relationships that other regression algorithms, such as linear regression and ridge/lasso regression, may miss. Decision tree regressor is also relatively simple to understand and interpret, which can be beneficial in situations where model transparency and explainability are important.\n",
        "\n",
        "*  Furthermore, decision tree regressor can handle missing values and outliers, and it is resistant to overfitting, especially when used with techniques such as pruning and ensemble methods like random forest.\n",
        "\n",
        "*  In summary, decision tree regressor is a powerful tool for regression analysis that can capture complex, nonlinear relationships in the data, is easy to interpret and explain, and is robust to outliers and missing data. Therefore, it is important to learn about decision tree regressor as it can provide better results than other regression techniques in certain scenarios.\n",
        "\n",
        "\n",
        "**Q2.when to apply decision for the best result in regression?**\n",
        "\n",
        "*  Decision tree regressor can be applied in regression problems when the relationship between the independent variables and dependent variable is nonlinear and complex. It is particularly useful when there are interactions between the variables and it is difficult to model them using linear models like linear regression or support vector regression.\n",
        "\n",
        "*  Decision tree regressor is also useful when the data has both continuous and categorical variables, as it can handle both types of variables. Additionally, it can handle missing values and outliers in the data.\n",
        "\n",
        "*  Moreover, decision tree regressor provides a clear interpretation of the model and the decision rules used to predict the target variable, making it easier to explain the results to non-technical stakeholders.\n",
        "\n",
        "*  Overall, decision tree regressor can be a good choice for regression problems when the data is complex and has both continuous and categorical variables, and when interpretability of the model is important.\n"
      ],
      "metadata": {
        "id": "s09gr91etQ80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Random Forest Regressor:**\n",
        "\n",
        "Random forest regression is an ensemble learning method that uses decision trees to perform regression. It is a widely used machine learning technique due to its high accuracy and ability to handle large datasets with high dimensionality.\n",
        "\n",
        "Here are the steps involved in random forest regression:\n",
        "\n",
        "*  Randomly select a subset of features from the given dataset.\n",
        "*  Build a decision tree based on the selected features.\n",
        "*  Repeat the above two steps multiple times to build a collection of decision trees.\n",
        "*  Combine the predictions from all the decision trees to get the final prediction.\n",
        "\n",
        "The random forest algorithm is called an \"ensemble\" method because it combines multiple decision trees to make a final prediction. Each decision tree is trained on a different subset of the data, using a different set of randomly selected features. This helps to reduce overfitting and improve the generalization performance of the model.\n",
        "\n",
        "*Here's an example code snippet in Python for performing random forest regression:*\n",
        "\n",
        "```\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_dataset()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Initialize the random forest regressor with hyperparameters\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
        "\n",
        "# Train the random forest regressor on the training set\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "```\n",
        "\n",
        "In the code above, we first load the dataset and split it into training and testing sets. Then we initialize a RandomForestRegressor object with hyperparameters such as the number of trees (n_estimators) and maximum depth of each tree (max_depth). We fit the regressor on the training set, make predictions on the testing set, and calculate the mean squared error as a performance metric.\n",
        "\n",
        "##**Questions Related to Random Forest Regressor:**\n",
        "\n",
        "\n",
        "**Q1. Why we have to know about random forest regression still there are many like decision tree , boosting techniques?**\n",
        "\n",
        "Random forest regression has several advantages over other regression techniques such as decision trees and boosting methods:\n",
        "\n",
        "*  Random forest regression reduces the risk of overfitting compared to decision trees by constructing multiple trees using random subsets of the data and features.\n",
        "\n",
        "*  Random forest regression can handle a large number of input features, including both numerical and categorical data.\n",
        "\n",
        "*  Random forest regression can handle missing data by imputing missing values based on available data.\n",
        "\n",
        "*  Random forest regression can provide feature importance ranking, which is useful for identifying the most important variables for predicting the target variable.\n",
        "\n",
        "*  Random forest regression can provide more accurate predictions compared to decision trees and other regression methods, especially for large and complex datasets.\n",
        "\n",
        "Therefore, it is important to know about random forest regression as it can provide more accurate and robust predictions compared to other regression techniques, especially for large and complex datasets with many input features.\n",
        "\n",
        "\n",
        "**Q2. In which scenario we have to go with the random forest regression?**\n",
        "\n",
        "Random forest regression is a powerful technique in machine learning that is used to solve regression problems. It is a combination of multiple decision trees that helps to achieve better accuracy and prevent overfitting. Here are some scenarios where random forest regression could be a good choice:\n",
        "\n",
        "*   **High dimensional dataset:** Random forest regression is effective in high dimensional datasets. It can handle a large number of features and still provide accurate results.\n",
        "\n",
        "*  **Non-linear relationships:** When there are non-linear relationships between the dependent and independent variables, random forest regression can perform well as it can capture complex interactions between the variables.\n",
        "\n",
        "*  **Outlier detection:** Random forest regression can handle outliers effectively by taking a median of multiple decision trees.\n",
        "\n",
        "*  **Missing values:** Random forest regression can handle missing values in the dataset. It uses only the available data to build each tree, and the missing values do not impact the performance of the model.\n",
        "\n",
        "*  **Robustness:** Random forest regression is a robust technique that is not easily affected by noise or irrelevant features in the dataset.\n",
        "\n",
        "Overall, random forest regression can be a good choice when there is a large amount of data with many features, and the relationship between the variables is complex and non-linear. It can also be a good choice when there are missing values or outliers in the dataset."
      ],
      "metadata": {
        "id": "D471I6jn_cU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Boosting Techniques:**\n",
        "\n",
        "###**Introduction:**\n",
        "\n",
        "Boosting is a machine learning technique that combines several weak models to create a strong predictive model. In boosting, the models are built sequentially, with each subsequent model attempting to correct the errors of the previous model. Boosting algorithms are a type of ensemble learning method, which involves combining several models to improve the overall performance of the model.\n",
        "\n",
        " *  The basic idea behind boosting is to train several weak models, such as decision trees, with different subsets of the training data, and combine their predictions to create a more accurate final model. The weak models are trained iteratively, with each subsequent model trained to improve the predictions of the previous model.\n",
        "\n",
        " *  Boosting algorithms typically work by assigning weights to each observation in the training set, with more weight given to observations that were misclassified by the previous model. This helps the subsequent models to focus on the observations that are difficult to classify, improving the overall accuracy of the model.\n",
        "\n",
        " *  There are several popular boosting algorithms, including AdaBoost, Gradient Boosting, XGBoost, and CatBoost. Each algorithm has its own strengths and weaknesses, and the choice of algorithm will depend on the specific problem and dataset.\n",
        "\n",
        " *  Boosting is a powerful technique for improving the accuracy of machine learning models, and is widely used in applications such as image and speech recognition, fraud detection, and recommender systems.\n",
        "\n",
        "###**Types of Boosting Techniques:**\n",
        "\n",
        "There are mainly four types of boosting techniques.\n",
        "\n",
        "*  **AdaBoost** - AdaBoost is a boosting algorithm that works by iteratively training weak classifiers on different weighted versions of the training data. Each weak classifier is then combined to create a strong classifier that can make accurate predictions. AdaBoost is relatively simple to implement and can be effective on a wide range of datasets, but it can be sensitive to noisy data.\n",
        "\n",
        "*  **XGBoost** - XGBoost is an optimized version of gradient boosting that uses a more efficient gradient boosting algorithm and includes additional regularization techniques to prevent overfitting. It is particularly useful for large datasets with many features, and is known for its high accuracy and speed.\n",
        "\n",
        "*  **CatBoost** - CatBoost is a gradient boosting algorithm that is specifically designed to handle categorical variables in the data. It uses an innovative algorithm to handle categorical features and can automatically handle missing values. CatBoost is particularly useful for datasets with a mix of numerical and categorical variables.\n",
        "\n",
        "*  **Gradient Boosting**- Gradient boosting is a general-purpose boosting algorithm that works by iteratively training weak models on the residuals of the previous models. It can handle a wide range of loss functions and is particularly useful for regression problems.\n",
        "\n",
        "In summary, AdaBoost is a simple and effective boosting algorithm, XGBoost is optimized for large datasets and includes additional regularization techniques, CatBoost is designed to handle categorical variables, and Gradient Boosting is a general-purpose boosting algorithm that is particularly useful for regression problems. The choice of algorithm will depend on the specific characteristics of the dataset and the problem at hand.\n",
        "\n",
        "\n",
        "###**Boosting Techniques With Implementation:**\n",
        "\n",
        "**XGBoost:**\n",
        "\n",
        "(Extreme Gradient Boosting) is a popular boosting algorithm used for both regression and classification tasks. It is known for its fast execution speed and high accuracy.\n",
        "\n",
        "Here is the step-by-step algorithm behind XGBoost:\n",
        "\n",
        "*  Initialize the model with a constant value (usually the mean of the target variable).\n",
        "*  Fit a decision tree to the data using the gradient descent algorithm to minimize the loss function.\n",
        "*  Evaluate the performance of the tree and calculate the residuals (the difference between the predicted and actual values).\n",
        "*  Fit another decision tree to the residuals and add it to the previous tree to update the predictions.\n",
        "*  Repeat steps 3 and 4 until the desired number of trees is reached or until the residuals are close to zero.\n",
        "*  The final prediction is the sum of the predicted values from all the trees.\n",
        "\n",
        "Here is an example of how to implement XGBoost in Python for a classification problem using the breast cancer dataset from scikit-learn:\n",
        "\n",
        "```\n",
        "# import necessary libraries and load the dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# initialize the XGBoost classifier with default hyperparameters\n",
        "model = XGBClassifier()\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "In this example, we first load the breast cancer dataset and split it into train and test sets. Then, we initialize an instance of the XGBClassifier class and fit it to the training data. Finally, we make predictions on the test data and evaluate the accuracy of the model using the accuracy_score function from scikit-learn.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**AdaBoost:**\n",
        "\n",
        "(Adaptive Boosting) is another popular boosting algorithm used in machine learning. AdaBoost is similar to XGBoost in that it combines several weak learners to form a strong learner. However, AdaBoost places greater emphasis on misclassified samples in each round of boosting.\n",
        "\n",
        "The following are the basic steps of the AdaBoost algorithm:\n",
        "\n",
        "*  Initialize weights for all training samples.\n",
        "\n",
        "*  Train a weak learner (a decision tree with a maximum depth of 1, also known as a \"stump\") on the training data.\n",
        "\n",
        "*  Calculate the weighted error rate of the weak learner.\n",
        "\n",
        "*  Calculate the weight of the weak learner based on its error rate.\n",
        "\n",
        "*  Update the weights of the training samples based on their correct or incorrect classification by the weak learner.\n",
        "\n",
        "*  Repeat steps 2-5 until the desired number of weak learners have been trained.\n",
        "\n",
        "*  Combine the weak learners into a strong learner by weighting their predictions based on their individual weight.\n",
        "\n",
        "Here's an example of using the AdaBoost algorithm in Python:\n",
        "\n",
        "```\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Initialize the AdaBoost classifier with a decision tree stump\n",
        "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=1)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "```\n",
        "In this example, we generate a synthetic dataset and split it into training and testing sets. We then initialize an AdaBoost classifier with a decision tree stump as the base estimator and train it on the training data. Finally, we make predictions on the testing data and calculate the accuracy score of the classifier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**CatBoost:**\n",
        " (Categorical boosting) is a popular boosting algorithm used for both classification and regression tasks.\n",
        " \n",
        "Here is an explanation of CatBoost along with an example code:\n",
        "\n",
        "CatBoost:\n",
        "\n",
        "CatBoost is an open-source gradient boosting algorithm developed by Yandex, a Russian search engine company. CatBoost stands for \"Categorical Boosting\" because it is designed to handle categorical features in data.\n",
        "\n",
        "The main features of CatBoost are:\n",
        "\n",
        "*  It can handle categorical variables without any preprocessing.\n",
        "*  It has built-in handling of missing values.\n",
        "*  It provides advanced visualization tools for understanding the model.\n",
        "*  It is computationally efficient and can handle large datasets.\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "*  Input the dataset and define the target variable.\n",
        "*  Split the dataset into training and validation sets.\n",
        "*  Define the hyperparameters for the CatBoost model, such as the number of iterations, learning rate, and depth of the trees.\n",
        "*  Train the CatBoost model on the training set using the defined hyperparameters.\n",
        "*  Evaluate the model on the validation set using appropriate evaluation metrics, such as accuracy or mean squared error.\n",
        "*  Tune the hyperparameters using grid search or random search to improve the model performance.\n",
        "*  Test the final model on the test set to evaluate its performance.\n",
        "\n",
        "Here is an example code in Python that demonstrates how to use CatBoost for a binary classification problem:\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Define the target variable\n",
        "target = 'class'\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data, train_target, val_target = train_test_split(data.drop(target, axis=1), data[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CatBoost model with default hyperparameters\n",
        "model = CatBoostClassifier()\n",
        "\n",
        "# Train the CatBoost model\n",
        "model.fit(train_data, train_target)\n",
        "\n",
        "# Predict the target variable for the validation set\n",
        "pred_target = model.predict(val_data)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(val_target, pred_target)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "```\n",
        "\n",
        "In this code, we first load the dataset and define the target variable. Then, we split the data into training and validation sets using the train_test_split() function. Next, we define the CatBoost model with default hyperparameters and train it on the training set using the fit() function. Finally, we predict the target variable for the validation set using the predict() function and evaluate the accuracy of the model using the accuracy_score() function.\n",
        "\n",
        "\n",
        "\n",
        "**Gradient Boosting:**\n",
        "It is a type of boosting algorithm used in supervised learning for regression and classification tasks. It builds the model in a stage-wise fashion and generalizes it by allowing optimization of an arbitrary differentiable loss function.\n",
        "\n",
        "Here is the step-by-step algorithm behind Gradient Boosting:\n",
        "\n",
        "*  Initialize the model with a constant value, usually the mean value of the target variable.\n",
        "*  Train the model using the training dataset.\n",
        "*  Calculate the residuals, which is the difference between the predicted and actual target values.\n",
        "*  Fit a new decision tree model to the residuals and add it to the current model.\n",
        "*  Repeat steps 3-4 until the desired number of trees is reached or until the residuals cannot be reduced any further.\n",
        "*  Predict the target variable using the final model.\n",
        "\n",
        "The code for Gradient Boosting in Python using the scikit-learn library would look something like this:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Initialize the model with default hyperparameters\n",
        "model = GradientBoostingRegressor()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target variable using the trained model\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "Here, X_train and y_train are the training data features and target variable, respectively, and X_test is the test data features. The predict() method is used to generate predictions for the test data.\n",
        "\n",
        "\n",
        "##**Questions Related to Boosting Techniques:**\n",
        "\n",
        "**Q1. why we have to know about boosting techniques still there are ridge and lasso , elastic net regression exists?**\n",
        "\n",
        "*  Boosting techniques are often used in ensemble learning, where multiple models are combined to create a stronger model. Unlike regularization techniques such as ridge and lasso regression, boosting methods focus on improving the performance of the model by sequentially adding models that correct the errors of the previous models. This helps to reduce bias and variance and can lead to better predictive accuracy, especially when dealing with complex and non-linear relationships between the features and the target variable.\n",
        "\n",
        "*  Boosting methods also have the advantage of being able to handle a wide variety of data types and model architectures, including both linear and non-linear models. They can be applied to a range of problems, including classification, regression, and ranking, and can be used with a variety of different loss functions.\n",
        "\n",
        "*  Overall, boosting techniques are an important tool in the machine learning toolbox, offering a powerful and flexible approach to building accurate and robust models. While regularization techniques such as ridge and lasso regression are also useful, they may not always be sufficient to achieve the desired level of performance, especially in complex and high-dimensional datasets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Q2. what is the difference between gradient and xg boosting?**\n",
        "\n",
        "Both Gradient Boosting and XGBoost are boosting techniques used for building ensembles of decision trees. The main differences between the two are:\n",
        "\n",
        "*   Regularization techniques: XGBoost applies a more advanced form of regularization called \"L1 regularization\" (Lasso) and \"L2 regularization\" (Ridge), while Gradient Boosting uses \"shrinkage\" and \"early stopping\" techniques for regularization.\n",
        "\n",
        "*   Speed and scalability: XGBoost is faster and more scalable than Gradient Boosting due to its implementation of parallel processing, cache optimization, and automatic pruning of decision trees.\n",
        "\n",
        "*   Handling missing values: XGBoost can handle missing values in the data, whereas Gradient Boosting requires that missing values be preprocessed.\n",
        "\n",
        "*   Tree splitting: XGBoost uses \"approximate greedy algorithm\" for tree splitting, while Gradient Boosting uses \"exact greedy algorithm\" which can be slower for large datasets.\n",
        "\n",
        "In summary, XGBoost is faster, more scalable, and more accurate in handling missing values, while Gradient Boosting can be more accurate in certain cases and has a simpler implementation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lSaZJpQbymHq"
      }
    }
  ]
}